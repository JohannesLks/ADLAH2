"""
Feature extraction and normalization for RL agent observations.
"""
import math
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import xxhash
import zlib

from .config import agent_config
from .connection_tracker import IPProfile

log = logging.getLogger(__name__)

# --- Feature Configuration based on the new plan ---

# 1. Curated Base Features (Normalized)
BASE_NUMERIC_FEATURES = [
    "FLOW.duration",
    "FLOW.bytes_toserver",
    "FLOW.bytes_toclient",
    "IP.ttl",
    "TCP.hdr_len",
    "TCP.window",
    # additional relevant numerics from appendix (implemented)
    "IP.tot_len",
    "IP.hdr_len",
    "FLOW.min_rtt",
    "TCP.ack_seq",
    "IP.protocol"
]

# 2. Enriched Categorical Features (Hashed One-Hot) and other categoricals
HASHED_CATEGORICAL_FEATURES = {
    # Feature Name: (dot.path.to.value, k_dimensions)
    "src_as_name": ("_source.src_ip_geoasn.as.name", 64),
    "src_cc": ("_source.src_ip_geoasn.as.country_code", 32),
    "tcp_flag_pattern": (None, 32), # Generated by function
    "flow_reason": ("FLOW.reason", 16),
    "flow_state": ("FLOW.state", 16),
    # additional hashed categoricals implemented
    "event_type": ("event_type", 16),
    "proto": ("proto", 8),
    "http_method": ("http.request.method", 16),
    "user_agent_name": ("user_agent.name", 32),
    "ct_status": ("ct_status", 8),
    "payload_sha1": ("FLOW.payload_sha1", 64),
}
# Special numeric categorical feature
ASN_NUMERIC_FEATURE = "_source.src_ip_geoasn.as.number"

# Port categories for classic one-hot encoding
WELL_KNOWN_PORTS = {22, 23, 80, 443, 5060, 123}
PORT_CATEGORIES = ["unknown", "well_known", "registered", "dynamic"] + [f"port_{p}" for p in WELL_KNOWN_PORTS]

# 3. Conditional Payload Features (Normalized & Reduced)
PAYLOAD_SCALAR_FEATURES = [
    "payload_len", "payload_entropy", "non_printable_ratio", "compress_ratio"
]
PAYLOAD_VECTOR_FEATURES = {
    "_bigram_sketch": 256,
    "_byte_histogram": 64
}
PAYLOAD_DIM = len(PAYLOAD_SCALAR_FEATURES) + sum(PAYLOAD_VECTOR_FEATURES.values())

# 4. Short-term Behavioral History (from IPProfile, Normalized)
HISTORY_FEATURES = [
    "ev_5m", "ports_5m", "protos_5m", "targets_5m", "syn_ratio_5m"
]

# 5. Time Features (sin/cos of event timestamp)
TIME_FEATURES_DIM = 2


# --- Helper Functions ---

def _get_value_by_path(doc: Dict[str, Any], path: str) -> Optional[Any]:
    """Safely get a nested value from a dict using a dot-separated path."""
    keys = path.split('.')
    value = doc
    for key in keys:
        if isinstance(value, dict):
            value = value.get(key)
        else:
            return None
    return value

def _hashed_one_hot(value: str, k: int, seed: int = 42) -> np.ndarray:
    """Hash a string into a 1-hot vector of size k."""
    vec = np.zeros(k, dtype=np.float32)
    if not isinstance(value, str) or not value:
        return vec
    idx = xxhash.xxh32(value.encode(), seed=seed).intdigest() % k
    vec[idx] = 1.0
    return vec

def _compress_ratio(payload_bytes: bytes, limit: int = 2048) -> float:
    """Compression ratio (compressed_len / raw_len)."""
    if not payload_bytes:
        return 0.0
    raw = payload_bytes[:limit]
    try:
        comp = zlib.compress(raw)
        return float(len(comp)) / max(1.0, float(len(raw)))
    except zlib.error:
        return 0.0

def _get_tcp_flag_pattern(doc: Dict) -> str:
    """Create a bitmask-like string from TCP flags."""
    flags = []
    # Use flat flag fields as listed in the dataset
    if _get_value_by_path(doc, "TCP.syn"): flags.append("SYN")
    if _get_value_by_path(doc, "TCP.ack"): flags.append("ACK")
    if _get_value_by_path(doc, "TCP.rst"): flags.append("RST")
    if _get_value_by_path(doc, "TCP.psh"): flags.append("PSH")
    if _get_value_by_path(doc, "TCP.fin"): flags.append("FIN")
    return "_".join(flags) if flags else "NONE"

def _get_port_category_vector(dest_port: Optional[int]) -> np.ndarray:
    """Create a one-hot vector for the destination port category."""
    vec = np.zeros(len(PORT_CATEGORIES), dtype=np.float32)
    if dest_port is None:
        vec[PORT_CATEGORIES.index("unknown")] = 1.0
        return vec

    if dest_port in WELL_KNOWN_PORTS:
        vec[PORT_CATEGORIES.index(f"port_{dest_port}")] = 1.0
    elif 1 <= dest_port <= 1023:
        vec[PORT_CATEGORIES.index("well_known")] = 1.0
    elif 1024 <= dest_port <= 49151:
        vec[PORT_CATEGORIES.index("registered")] = 1.0
    else: # 49152-65535
        vec[PORT_CATEGORIES.index("dynamic")] = 1.0
    return vec


class FeatureNormalizer:
    """Online feature normalization using Welford's algorithm."""
    def __init__(self, feature_name: str, mean: float = 0.0, std: float = 1.0, count: int = 0):
        self.feature_name = feature_name
        self.mean = float(mean)
        self.M2 = (std ** 2) * max(count - 1, 0) if count > 0 else 0.0
        self.count = int(count)

    @property
    def std(self) -> float:
        if self.count < 2:
            return 1.0
        return math.sqrt(self.M2 / self.count) or 1.0

    def update(self, value: float):
        self.count += 1
        delta = value - self.mean
        self.mean += delta / self.count
        delta2 = value - self.mean
        self.M2 += delta * delta2

    def normalize(self, value: float) -> float:
        # Don't update stats for normalization, update is explicit
        if self.count < 2:
            return 0.0 # Return 0 if not enough data, avoiding large values
        
        normalized = (value - self.mean) / self.std
        return np.clip(normalized, -5.0, 5.0) # Clip to a reasonable range

    def to_dict(self) -> Dict:
        return {"mean": self.mean, "std": self.std, "count": self.count}

    @classmethod
    def from_dict(cls, feature_name: str, data: Dict) -> "FeatureNormalizer":
        return cls(feature_name, data.get("mean", 0.0), data.get("std", 1.0), data.get("count", 0))


class FeatureExtractor:
    """Extracts and normalizes features according to the new specification."""
    def __init__(self):
        self.normalizers: Dict[str, FeatureNormalizer] = {}
        self.stats_path = agent_config.cache_dir / "feature_stats_v2.json"
        self.load_stats()
        
        # Static part of the feature dimension
        # Add one dynamic runtime feature: time_since_last_min (in minutes)
        self.base_dim = (
            len(BASE_NUMERIC_FEATURES) +
            1 + # ASN numeric feature
            sum(k for _, k in HASHED_CATEGORICAL_FEATURES.values()) +
            len(PORT_CATEGORIES) +
            len(HISTORY_FEATURES) +
            TIME_FEATURES_DIM +
            1  # time_since_last_min
        )
        log.info(f"Base feature dimension: {self.base_dim}")
        log.info(f"Payload feature dimension: {PAYLOAD_DIM}")

    @property
    def feature_dim(self) -> int:
        """Returns the total feature dimension, assuming payload is present."""
        return self.base_dim + PAYLOAD_DIM

    def extract(self, doc: Dict, ip_profile: IPProfile, time_since_last_min: float = 0.0) -> np.ndarray:
        """Main feature extraction function."""
        features = []
        
        # 1. Base Numeric Features
        for feat_path in BASE_NUMERIC_FEATURES:
            value = float(_get_value_by_path(doc, feat_path) or 0.0)
            features.append(self._normalize_and_update(feat_path, value))
            
        # 2. Enriched Categorical & Time Features
        # ASN as a numeric feature
        asn_val = float(_get_value_by_path(doc, ASN_NUMERIC_FEATURE) or 0.0)
        features.append(self._normalize_and_update(ASN_NUMERIC_FEATURE, asn_val))
        
        # Hashed one-hot features
        for name, (path, k) in HASHED_CATEGORICAL_FEATURES.items():
            if name == "tcp_flag_pattern":
                value = _get_tcp_flag_pattern(doc)
            else:
                value = str(_get_value_by_path(doc, path) or "")
            features.extend(_hashed_one_hot(value, k))
            
        # Port category (classic one-hot)
        # Try multiple locations for destination port
        dest_port = _get_value_by_path(doc, "FLOW.dest_port")
        if dest_port is None:
            dest_port = _get_value_by_path(doc, "dest_port")
        if dest_port is None:
            dest_port = _get_value_by_path(doc, "TCP.dest_port")
        if dest_port is None:
            dest_port = _get_value_by_path(doc, "UDP.dest_port")
        features.extend(_get_port_category_vector(dest_port))
        
        # 5. Time Features (from event timestamp)
        ts_str = _get_value_by_path(doc, "@timestamp")
        timestamp = datetime.fromisoformat(ts_str.replace('Z', '+00:00')) if ts_str else datetime.now(timezone.utc)
        seconds_in_day = timestamp.hour * 3600 + timestamp.minute * 60 + timestamp.second
        features.extend([
            math.sin(2 * math.pi * seconds_in_day / 86400),
            math.cos(2 * math.pi * seconds_in_day / 86400)
        ])

        # 5b. Dynamic runtime feature provided by the caller (capped minutes since last event)
        features.append(self._normalize_and_update("time_since_last_min", float(time_since_last_min)))
        
        # 4. Historical Features
        hist_features = ip_profile.get_historical_features()
        for feat_name in HISTORY_FEATURES:
            value = hist_features.get(feat_name, 0.0)
            features.append(self._normalize_and_update(feat_name, value))

        # 3. Conditional Payload Features
        payload_str = _get_value_by_path(doc, "FLOW.payload_str")
        if payload_str:
            payload_bytes = bytes.fromhex(payload_str)
            payload_len = len(payload_bytes)
        else:
            payload_bytes = b''
            payload_len = 0

        if payload_len > 0:
            # Scalar payload features
            byte_counts = {i: 0 for i in range(256)}
            for byte in payload_bytes: byte_counts[byte] += 1
            entropy = -sum((c / payload_len) * math.log2(c / payload_len) for c in byte_counts.values() if c > 0)
            non_printable = sum(1 for b in payload_bytes if b < 32 or b > 126)
            
            payload_scalars = {
                "payload_len": float(payload_len),
                "payload_entropy": entropy,
                "non_printable_ratio": non_printable / payload_len,
                "compress_ratio": _compress_ratio(payload_bytes)
            }
            for name in PAYLOAD_SCALAR_FEATURES:
                value = payload_scalars.get(name, 0.0)
                features.append(self._normalize_and_update(name, value))

            # Vector payload features (placeholders disabled: use zeros until real sketches are implemented)
            features.extend(np.zeros(PAYLOAD_VECTOR_FEATURES["_bigram_sketch"], dtype=np.float32))
            features.extend(np.zeros(PAYLOAD_VECTOR_FEATURES["_byte_histogram"], dtype=np.float32))
        else:
            # Append a zero vector if no payload
            features.extend(np.zeros(PAYLOAD_DIM, dtype=np.float32))
            
        return np.array(features, dtype=np.float32)

    def _normalize_and_update(self, name: str, value: float) -> float:
        """Normalize value using the dedicated normalizer and then update it."""
        if name not in self.normalizers:
            self.normalizers[name] = FeatureNormalizer(name)
        
        normalized_value = self.normalizers[name].normalize(value)
        self.normalizers[name].update(value) # Update after getting the normalized value
        return normalized_value

    def load_stats(self):
        if self.stats_path.exists():
            try:
                with open(self.stats_path, 'r') as f:
                    data = json.load(f)
                    for name, stats in data.items():
                        self.normalizers[name] = FeatureNormalizer.from_dict(name, stats)
                log.info(f"Loaded feature statistics for {len(self.normalizers)} features from {self.stats_path}")
            except Exception as e:
                log.error(f"Failed to load feature stats: {e}")

    def save_stats(self):
        try:
            with open(self.stats_path, 'w') as f:
                json.dump({name: norm.to_dict() for name, norm in self.normalizers.items()}, f, indent=2)
            log.debug(f"Saved feature statistics for {len(self.normalizers)} features to {self.stats_path}")
        except Exception as e:
            log.error(f"Failed to save feature stats: {e}")

# --- Global Instance and Convenience Functions ---
feature_extractor = FeatureExtractor()

def extract_features(doc: Dict, ip_profile: IPProfile, time_since_last_min: float = 0.0) -> np.ndarray:
    return feature_extractor.extract(doc, ip_profile, time_since_last_min)

def save_feature_stats():
    feature_extractor.save_stats()

def get_feature_dim() -> int:
    return feature_extractor.feature_dim